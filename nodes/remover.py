from PIL import ImageOps, ImageFilter
import torch
import numpy as np
from ..utils import cropimage, padimage, padmask, tensor2pil, pil2tensor, cropimage, pil2comfy
from ..lama import model
from torchvision import transforms


class LamaRemover:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "images": ("IMAGE",),
                "masks": ("MASK",),
                "mask_threshold": ("INT", {"default": 250, "min": 0, "max": 255, "step": 1, "display": "slider"}),
                "gaussblur_radius": ("INT", {"default": 8, "min": 0, "max": 20, "step": 1, "display": "slider"}),
                "invert_mask": ("BOOLEAN", {"default": False}),
            },
        }

    CATEGORY = "LamaRemover"
    RETURN_NAMES = ("images",)
    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "lama_remover"

    def _normalize_image(self, image_tensor):
        """
        å°‡åœ–åƒå¼µé‡æ­¸ä¸€åŒ–åˆ° [0, 1] ç¯„åœ
        TensorRT æ¨¡å‹æœŸæœ›è¼¸å…¥åœ¨ [0, 1] ç¯„åœå…§
        """
        if image_tensor.max() > 1.0:
            # å¦‚æœåœ–åƒåœ¨ [0, 255] ç¯„åœå…§ï¼Œæ­¸ä¸€åŒ–åˆ° [0, 1]
            image_tensor = image_tensor / 255.0

        # ç¢ºä¿åœ¨ [0, 1] ç¯„åœå…§
        image_tensor = torch.clamp(image_tensor, 0.0, 1.0)
        return image_tensor

    def _normalize_mask(self, mask_tensor):
        """
        å°‡é®ç½©å¼µé‡æ­¸ä¸€åŒ–åˆ° [0, 1] ç¯„åœ
        ç¢ºä¿é®ç½©ç‚ºå–®é€šé“ï¼Œä¸¦ä¸”æ•¸å€¼åœ¨æ­£ç¢ºç¯„åœå…§
        """
        if mask_tensor.max() > 1.0:
            mask_tensor = mask_tensor / 255.0

        # ç¢ºä¿åœ¨ [0, 1] ç¯„åœå…§
        mask_tensor = torch.clamp(mask_tensor, 0.0, 1.0)

        # ç¢ºä¿é®ç½©æ˜¯å–®é€šé“çš„
        if mask_tensor.dim() == 4 and mask_tensor.shape[1] != 1:
            # å¦‚æœæ˜¯å¤šé€šé“ï¼Œå–ç¬¬ä¸€å€‹é€šé“
            mask_tensor = mask_tensor[:, :1, :, :]
        elif mask_tensor.dim() == 3:
            # å¦‚æœæ˜¯ 3Dï¼Œå¢åŠ é€šé“ç¶­åº¦
            mask_tensor = mask_tensor.unsqueeze(1)

        return mask_tensor

    def _denormalize_output(self, output_tensor):
        """
        å°‡è¼¸å‡ºå¼µé‡å¾ [0, 1] ç¯„åœåæ­¸ä¸€åŒ–ï¼Œæº–å‚™ç”¨æ–¼é¡¯ç¤º
        """
        # ç¢ºä¿è¼¸å‡ºåœ¨ [0, 1] ç¯„åœå…§
        output_tensor = torch.clamp(output_tensor, 0.0, 1.0)
        return output_tensor

    def _prepare_tensors_for_trt(self, image_tensor, mask_tensor):
        """
        ç‚º TensorRT æ¨ç†æº–å‚™å¼µé‡
        ç¢ºä¿æ­£ç¢ºçš„å½¢ç‹€ã€è¨­å‚™å’Œæ•¸æ“šé¡å‹
        """
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # ç¢ºä¿å¼µé‡åœ¨æ­£ç¢ºçš„è¨­å‚™ä¸Š
        image_tensor = image_tensor.to(device, dtype=torch.float32)
        mask_tensor = mask_tensor.to(device, dtype=torch.float32)

        # ç¢ºä¿æ˜¯ 4D å¼µé‡ (batch, channels, height, width)
        if image_tensor.dim() == 3:
            image_tensor = image_tensor.unsqueeze(0)
        if mask_tensor.dim() == 3:
            mask_tensor = mask_tensor.unsqueeze(0)

        # æ­¸ä¸€åŒ–
        image_tensor = self._normalize_image(image_tensor)
        mask_tensor = self._normalize_mask(mask_tensor)

        # èª¿è©¦è³‡è¨Š
        print(f"TRT è¼¸å…¥åœ–åƒç¯„åœ: [{image_tensor.min().item():.4f}, {image_tensor.max().item():.4f}]")
        print(f"TRT è¼¸å…¥é®ç½©ç¯„åœ: [{mask_tensor.min().item():.4f}, {mask_tensor.max().item():.4f}]")
        print(f"TRT è¼¸å…¥åœ–åƒå½¢ç‹€: {image_tensor.shape}")
        print(f"TRT è¼¸å…¥é®ç½©å½¢ç‹€: {mask_tensor.shape}")

        return image_tensor, mask_tensor

    def lama_remover(self, images, masks, mask_threshold, gaussblur_radius, invert_mask):
        # åˆå§‹åŒ– TensorRT æ¨¡å‹ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ä»¥æé«˜æ•ˆç‡ï¼‰
        if not hasattr(self, '_mylama'):
            self._mylama = model.BigLama()
            print("âœ… TensorRT æ¨¡å‹å·²åˆå§‹åŒ–")

        ten2pil = transforms.ToPILImage()
        results = []

        for i, (image, mask) in enumerate(zip(images, masks)):
            print(f"\n--- è™•ç†ç¬¬ {i + 1} å¼µåœ–åƒ ---")

            ori_image = tensor2pil(image)
            print(f"åŸå§‹åœ–åƒå°ºå¯¸: {ori_image.size}")

            w, h = ori_image.size
            p_image = padimage(ori_image)
            pt_image = pil2tensor(p_image)

            mask = mask.unsqueeze(0)
            ori_mask = ten2pil(mask)
            ori_mask = ori_mask.convert('L')
            print(f"åŸå§‹é®ç½©å°ºå¯¸: {ori_mask.size}")

            p_mask = padmask(ori_mask)

            if p_mask.size != p_image.size:
                print("èª¿æ•´é®ç½©å°ºå¯¸")
                p_mask = p_mask.resize(p_image.size)

            # åè½‰é®ç½©
            if not invert_mask:
                p_mask = ImageOps.invert(p_mask)

            # é«˜æ–¯æ¨¡ç³Š
            p_mask = p_mask.filter(ImageFilter.GaussianBlur(radius=gaussblur_radius))

            # é®ç½©é–¾å€¼è™•ç†
            gray = p_mask.point(lambda x: 0 if x > mask_threshold else 255)
            pt_mask = pil2tensor(gray)

            # ç‚º TensorRT æº–å‚™å¼µé‡
            try:
                trt_image, trt_mask = self._prepare_tensors_for_trt(pt_image, pt_mask)

                # TensorRT æ¨ç†
                print("ğŸš€ é–‹å§‹ TensorRT æ¨ç†...")
                with torch.no_grad():
                    result = self._mylama(trt_image, trt_mask)

                # è™•ç†è¼¸å‡º
                result = self._denormalize_output(result)

                # ç§»é™¤æ‰¹æ¬¡ç¶­åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if result.dim() == 4 and result.shape[0] == 1:
                    result = result.squeeze(0)

                print(f"TRT è¼¸å‡ºç¯„åœ: [{result.min().item():.4f}, {result.max().item():.4f}]")
                print(f"TRT è¼¸å‡ºå½¢ç‹€: {result.shape}")

                # è½‰æ›ç‚º PIL åœ–åƒ
                img_result = ten2pil(result)

                # è£å‰ªåˆ°åŸå§‹å°ºå¯¸
                x, y = img_result.size
                if x > w or y > h:
                    img_result = cropimage(img_result, w, h)

                # è½‰æ›ç‚º ComfyUI æ ¼å¼
                i = pil2comfy(img_result)
                results.append(i)

                print(f"âœ… ç¬¬ {i + 1} å¼µåœ–åƒè™•ç†å®Œæˆ")

            except Exception as e:
                print(f"âŒ TensorRT æ¨ç†å¤±æ•—: {e}")
                print(f"éŒ¯èª¤è©³æƒ…: {type(e).__name__}")

                # éŒ¯èª¤è™•ç†ï¼šè¿”å›åŸå§‹åœ–åƒ
                print("ğŸ”„ è¿”å›åŸå§‹åœ–åƒä½œç‚ºéŒ¯èª¤è™•ç†")
                ori_result = pil2comfy(ori_image)
                results.append(ori_result)

        return (torch.cat(results, dim=0),)


class LamaRemoverIMG:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "images": ("IMAGE",),
                "masks": ("IMAGE",),
                "mask_threshold": ("INT", {"default": 250, "min": 0, "max": 255, "step": 1, "display": "slider"}),
                "gaussblur_radius": ("INT", {"default": 8, "min": 0, "max": 20, "step": 1, "display": "slider"}),
                "invert_mask": ("BOOLEAN", {"default": False}),
            },
        }

    CATEGORY = "LamaRemover"
    RETURN_NAMES = ("images",)
    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "lama_remover_IMG"

    def _normalize_image(self, image_tensor):
        """
        å°‡åœ–åƒå¼µé‡æ­¸ä¸€åŒ–åˆ° [0, 1] ç¯„åœ
        """
        if image_tensor.max() > 1.0:
            image_tensor = image_tensor / 255.0
        image_tensor = torch.clamp(image_tensor, 0.0, 1.0)
        return image_tensor

    def _normalize_mask(self, mask_tensor):
        """
        å°‡é®ç½©å¼µé‡æ­¸ä¸€åŒ–åˆ° [0, 1] ç¯„åœ
        """
        if mask_tensor.max() > 1.0:
            mask_tensor = mask_tensor / 255.0
        mask_tensor = torch.clamp(mask_tensor, 0.0, 1.0)

        if mask_tensor.dim() == 4 and mask_tensor.shape[1] != 1:
            mask_tensor = mask_tensor[:, :1, :, :]
        elif mask_tensor.dim() == 3:
            mask_tensor = mask_tensor.unsqueeze(1)

        return mask_tensor

    def _denormalize_output(self, output_tensor):
        """
        å°‡è¼¸å‡ºå¼µé‡åæ­¸ä¸€åŒ–
        """
        output_tensor = torch.clamp(output_tensor, 0.0, 1.0)
        return output_tensor

    def _prepare_tensors_for_trt(self, image_tensor, mask_tensor):
        """
        ç‚º TensorRT æ¨ç†æº–å‚™å¼µé‡
        """
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        image_tensor = image_tensor.to(device, dtype=torch.float32)
        mask_tensor = mask_tensor.to(device, dtype=torch.float32)

        if image_tensor.dim() == 3:
            image_tensor = image_tensor.unsqueeze(0)
        if mask_tensor.dim() == 3:
            mask_tensor = mask_tensor.unsqueeze(0)

        image_tensor = self._normalize_image(image_tensor)
        mask_tensor = self._normalize_mask(mask_tensor)

        print(f"TRT è¼¸å…¥åœ–åƒç¯„åœ: [{image_tensor.min().item():.4f}, {image_tensor.max().item():.4f}]")
        print(f"TRT è¼¸å…¥é®ç½©ç¯„åœ: [{mask_tensor.min().item():.4f}, {mask_tensor.max().item():.4f}]")

        return image_tensor, mask_tensor

    def lama_remover_IMG(self, images, masks, mask_threshold, gaussblur_radius, invert_mask):
        if not hasattr(self, '_mylama'):
            self._mylama = model.BigLama()
            print("âœ… TensorRT æ¨¡å‹å·²åˆå§‹åŒ–")

        ten2pil = transforms.ToPILImage()
        results = []

        for i, (image, mask) in enumerate(zip(images, masks)):
            print(f"\n--- è™•ç†ç¬¬ {i + 1} å¼µåœ–åƒ (IMG æ¨¡å¼) ---")

            ori_image = tensor2pil(image)
            print(f"åŸå§‹åœ–åƒå°ºå¯¸: {ori_image.size}")

            w, h = ori_image.size
            p_image = padimage(ori_image)
            pt_image = pil2tensor(p_image)

            mask = mask.movedim(0, -1).movedim(0, -1)
            ori_mask = ten2pil(mask)
            ori_mask = ori_mask.convert('L')
            print(f"åŸå§‹é®ç½©å°ºå¯¸: {ori_mask.size}")

            p_mask = padmask(ori_mask)

            if p_mask.size != p_image.size:
                print("èª¿æ•´é®ç½©å°ºå¯¸")
                p_mask = p_mask.resize(p_image.size)

            # åè½‰é®ç½©
            if not invert_mask:
                p_mask = ImageOps.invert(p_mask)

            # é«˜æ–¯æ¨¡ç³Š
            p_mask = p_mask.filter(ImageFilter.GaussianBlur(radius=gaussblur_radius))

            # é®ç½©é–¾å€¼è™•ç†
            gray = p_mask.point(lambda x: 0 if x > mask_threshold else 255)
            pt_mask = pil2tensor(gray)

            # ç‚º TensorRT æº–å‚™å¼µé‡
            try:
                trt_image, trt_mask = self._prepare_tensors_for_trt(pt_image, pt_mask)

                print("ğŸš€ é–‹å§‹ TensorRT æ¨ç†...")
                with torch.no_grad():
                    result = self._mylama(trt_image, trt_mask)

                result = self._denormalize_output(result)

                if result.dim() == 4 and result.shape[0] == 1:
                    result = result.squeeze(0)

                print(f"TRT è¼¸å‡ºç¯„åœ: [{result.min().item():.4f}, {result.max().item():.4f}]")

                img_result = ten2pil(result)

                x, y = img_result.size
                if x > w or y > h:
                    img_result = cropimage(img_result, w, h)

                i = pil2comfy(img_result)
                results.append(i)

                print(f"âœ… ç¬¬ {i + 1} å¼µåœ–åƒè™•ç†å®Œæˆ")

            except Exception as e:
                print(f"âŒ TensorRT æ¨ç†å¤±æ•—: {e}")
                ori_result = pil2comfy(ori_image)
                results.append(ori_result)

        return (torch.cat(results, dim=0),)


NODE_CLASS_MAPPINGS = {
    "LamaRemover": LamaRemover,
    "LamaRemoverIMG": LamaRemoverIMG
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LamaRemover": "Big lama Remover (TensorRT)",
    "LamaRemoverIMG": "Big lama Remover(IMG) (TensorRT)"
}